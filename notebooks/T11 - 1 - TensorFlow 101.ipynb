{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48caef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d891c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant([1,2,3,4,5])\n",
    "x2 = tf.constant([6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fcd0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_2:0\", shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "res = tf.multiply(x1, x2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92701ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 14 24 36 50]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(res))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f540fec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 14 24 36 50]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(res)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f48339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(log_device_placement = True)\n",
    "config = tf.ConfigProto(allow_soft_placement = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e1213",
   "metadata": {},
   "source": [
    "# Aprendizaje neuronal de las señales de tráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0db9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio as io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a58200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_data(data_directory):\n",
    "    dirs = [d for d in os.listdir(data_directory)\n",
    "            if os.path.isdir(os.path.join(data_directory,d))]\n",
    "    print(dirs)\n",
    "    \n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in dirs:\n",
    "        label_dir = os.path.join(data_directory, d)\n",
    "        file_names = [os.path.join(label_dir, f)\n",
    "                      for f in os.listdir(label_dir) \n",
    "                      if f.endswith(\".ppm\")]\n",
    "        \n",
    "        for f in file_names:\n",
    "            \n",
    "            images.append(io.imread(f))\n",
    "            images.append(int(d))\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee2f32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"../datasets/belgian/\"\n",
    "train_data_dir = os.path.join(main_dir, \"Training\")\n",
    "test_data_dir = os.path.join(main_dir, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e33c1104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042', '00043', '00044', '00045', '00046', '00047', '00048', '00049', '00050', '00051', '00052', '00053', '00054', '00055', '00056', '00057', '00058', '00059', '00060', '00061']\n"
     ]
    }
   ],
   "source": [
    "images, labels = load_ml_data(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "835850c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06cda901",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a395edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a4c72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "476e9845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[210, 249, 232],\n",
       "        [204, 249, 208],\n",
       "        [197, 198, 155],\n",
       "        ...,\n",
       "        [ 51,  60,  40],\n",
       "        [ 54,  64,  44],\n",
       "        [ 57,  66,  46]],\n",
       "\n",
       "       [[209, 250, 236],\n",
       "        [212, 255, 217],\n",
       "        [200, 196, 156],\n",
       "        ...,\n",
       "        [ 49,  57,  38],\n",
       "        [ 51,  59,  41],\n",
       "        [ 53,  60,  42]],\n",
       "\n",
       "       [[203, 246, 236],\n",
       "        [207, 246, 213],\n",
       "        [202, 192, 156],\n",
       "        ...,\n",
       "        [ 47,  53,  35],\n",
       "        [ 48,  54,  36],\n",
       "        [ 48,  55,  37]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  2,  22,  25],\n",
       "        [ 26,  56,  77],\n",
       "        [ 71, 140, 159],\n",
       "        ...,\n",
       "        [ 84,  77,  50],\n",
       "        [ 68,  66,  41],\n",
       "        [ 56,  64,  44]],\n",
       "\n",
       "       [[  0,  22,  32],\n",
       "        [ 30,  75, 106],\n",
       "        [ 87, 176, 198],\n",
       "        ...,\n",
       "        [ 86,  80,  52],\n",
       "        [ 68,  66,  41],\n",
       "        [ 55,  63,  42]],\n",
       "\n",
       "       [[  0,  32,  50],\n",
       "        [ 42, 101, 135],\n",
       "        [121, 217, 239],\n",
       "        ...,\n",
       "        [ 87,  80,  52],\n",
       "        [ 70,  68,  43],\n",
       "        [ 58,  66,  46]]], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af0fcdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  C_CONTIGUOUS : True\n",
       "  F_CONTIGUOUS : True\n",
       "  OWNDATA : True\n",
       "  WRITEABLE : True\n",
       "  ALIGNED : True\n",
       "  WRITEBACKIFCOPY : False\n",
       "  UPDATEIFCOPY : False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe7e9649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73200"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39679b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1560320d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9150.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.nbytes/images.itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0637a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5545705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1b56cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_signs = random.choice([range(0, len(labels)), 6])\n",
    "rand_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abac14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rand_signs)):\n",
    "    temp_im = images[rand_signs[i]]\n",
    "    plt.subplot(1,6,i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(temp_im)\n",
    "    plt.subplots_adjust(wspace = 0.5)\n",
    "    plt.show()\n",
    "    print(\"Forma:{0}, min:{1}, max:{2}\".format(temp_im.shape,\n",
    "                                               temp_im.min(),\n",
    "                                               temp_im.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "292e23bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223e5880080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_labels = set(labels)\n",
    "plt.figure(figsize=(16,16))\n",
    "i = 1\n",
    "for label in unique_labels:\n",
    "    temp_im = images[list(labels).index(label)]\n",
    "    plt.subplot(8,8, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Clase {0} ({1})\".format(label, list(labels).count(label)))\n",
    "    i +=1\n",
    "    plt.imshow(temp_im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd47e59",
   "metadata": {},
   "source": [
    "# Modelo de Red Neuronal con TensorFlow\n",
    "* Las imágenes no todas son del mismo tamaño\n",
    "* Hay 62 clases de imágenes (desde la 0 hasta la 61)\n",
    "* La distribución de señales de tráfico no es uniforme (algunas salen más veces que otras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9057ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d7f385f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-8abbc27c4524>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m9999\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "w = 9999 \n",
    "h = 9999\n",
    "for image in images:\n",
    "    if image.shape[0] < h:\n",
    "        h = image.shape[0]\n",
    "    if image.shape[1] < w:\n",
    "        w = image.shape[1]\n",
    "print(\"Tamaño mínimo: {0}x{1}\".format(h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8758a8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\51931\\anaconda3\\envs\\prueba\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000223F0602EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000223F0602EB8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000223F0602EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000223F0602EB8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000223EF9E4CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000223EF9E4CF8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000223EF9E4CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000223EF9E4CF8>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 30,30])\n",
    "y = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "\n",
    "images_flat = tf.contrib.layers.flatten(x)\n",
    "logits = tf.contrib.layers.fully_connected(images_flat, 62, tf.nn.relu)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits=logits))\n",
    "\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "final_pred = tf.argmax(logits,1)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(final_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec142c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
